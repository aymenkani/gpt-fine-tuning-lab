{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning GPT-2 Manually with PyTorch and Hugging Face Transformers",
        "",
        "This notebook is the **first phase** in a multi-stage project demonstrating my end-to-end skills in fine-tuning and aligning Large Language Models (LLMs) for a specific domain (e.g., code generation/problem-solving).",
        "",
        "My comprehensive project plan is to demonstrate proficiency across different LLM engineering paradigms:",
        "",
        "1.  **Fine-Tune Manually (This Lab):** I will start with the base GPT-2 model and manually implement the entire training loop using native **PyTorch**. This showcases a deep understanding of the underlying mechanisms of the training process, including manual setup of the optimizer, scheduler, and the forward/backward pass.",
        "2.  **Upload to Hugging Face (This Lab):** I will save and upload the manually fine-tuned model and tokenizer to my Hugging Face Hub account, making it publicly accessible.",
        "3.  **Refined Fine-Tuning (Next Lab):** In a separate notebook, I will load this fine-tuned model from the Hub and further train it using the streamlined **Hugging Face `transformers` `Trainer` class**. This demonstrates proficiency in using high-level, production-ready tools.",
        "4.  **Alignment using DPO (Final Lab):** Finally, I will take the model from the previous step and apply a sophisticated reinforcement learning technique, **Direct Preference Optimization (DPO)**, in a third lab to align its outputs with human preferences (e.g., for better code quality or safety)."
      ],
      "metadata": {
        "id": "intro-description"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Project Setup: Installing Dependencies",
        "",
        "I'm installing the necessary libraries, including the `transformers` library for the GPT-2 model and tokenizer, `datasets` for efficient data handling, and `accelerate` for easier multi-GPU/device management."
      ],
      "metadata": {
        "id": "setup-dependencies"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets accelerate -U"
      ],
      "metadata": {
        "id": "e_zB0n2R2-x9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Version Control Setup",
        "",
        "I'm ensuring Git LFS (Large File Storage) is installed. This is a crucial preparatory step for handling large model files when pushing them to the Hugging Face Hub."
      ],
      "metadata": {
        "id": "install-git-lfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt install git-lfs"
      ],
      "metadata": {
        "id": "Qn_S_pB4CjFh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Authenticating with Hugging Face Hub",
        "",
        "To upload my fine-tuned model and tokenizer later, I need to log into my Hugging Face account securely via the CLI."
      ],
      "metadata": {
        "id": "huggingface-login"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "id": "lU11727I3Cjs",
        "output_hidden": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Data Acquisition and Initial Load",
        "",
        "I'm using the powerful `datasets` library to load my custom coding problem dataset, which is currently stored in a CSV file."
      ],
      "metadata": {
        "id": "data-load-initial"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load the dataset from the uploaded file path\n",
        "ds = load_dataset(\"csv\", data_files=\"data/coding_dataset_1785725.csv\", split=\"train\")"
      ],
      "metadata": {
        "id": "f52b61D98Uo4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Custom Data Formatting for Causal Language Modeling (CLM)",
        "",
        "I define a function to format the dataset rows into a single `text` column with distinct separator tokens (`###`). This is the required input format for Causal Language Modeling to train the model to generate the solution and reasoning based on the problem statement."
      ],
      "metadata": {
        "id": "data-formatting-clm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_data(example):\n",
        "  # Concatenate all relevant columns into a single string for CLM\n",
        "  text = f\"Problem: {example['problem_statement']}\\n###Solution: {example['solution']}\\n###Reasoning: {example['reasoning']}\"\n",
        "  return {\"text\": text}\n",
        "\n",
        "# Apply the formatting function\n",
        "formatted = ds.map(format_data, remove_columns=ds.column_names)"
      ],
      "metadata": {
        "id": "H4oPjKjS87kU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Creating Train and Validation Subsets",
        "",
        "I'm performing a train/test split (80/20) and then selecting a small, manageable subset of the data (2500 training examples and 500 validation examples) for efficient local fine-tuning."
      ],
      "metadata": {
        "id": "train-val-split"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formatted_split = formatted[\"train\"].train_test_split(test_size=0.2)\n",
        "\n",
        "# Select a subset of the split datasets for demonstration purposes\n",
        "train_size = 2500\n",
        "test_size = 500\n",
        "\n",
        "# Ensure the selected size does not exceed the actual size of the split\n",
        "train_subset_size = min(train_size, len(formatted_split[\"train\"]))\n",
        "test_subset_size = min(test_size, len(formatted_split[\"test\"]))\n",
        "\n",
        "formatted_split[\"train\"] = formatted_split[\"train\"].select(range(train_subset_size))\n",
        "formatted_split[\"test\"] = formatted_split[\"test\"].select(range(test_subset_size))\n",
        "\n",
        "print(formatted_split)"
      ],
      "metadata": {
        "id": "tM95g4Vl6UGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Tokenization and Input Preparation",
        "",
        "I load the `gpt2` tokenizer, explicitly set the padding token to the EOS token, and tokenize the dataset. This creates the `input_ids` and `attention_mask` tensors required by the model, preparing the data for the PyTorch loop."
      ],
      "metadata": {
        "id": "tokenization"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "if tokenizer.pad_token is None:\n",
        "  tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_function(examples):\n",
        "  return tokenizer(\n",
        "    examples[\"text\"],\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=512\n",
        "  )\n",
        "\n",
        "tokenized_ds = formatted_split.map(tokenize_function, batched=True, remove_columns=formatted[\"train\"].column_names)"
      ],
      "metadata": {
        "id": "G07x2r0C9YtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Environment and Device Setup (PyTorch Manual Setup Phase)",
        "",
        "I'm explicitly setting up the environment, ensuring PyTorch uses the available GPU (`cuda`) for accelerated training, which is a standard practice in deep learning."
      ],
      "metadata": {
        "id": "device-setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForCausalLM\n",
        "import torch.nn.functional as F\n",
        "from accelerate import Accelerator\n",
        "\n",
        "# Check for CUDA and set device\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "id": "K1gT56N0N-8B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Initializing the Pre-trained GPT-2 Model",
        "",
        "I'm loading the base `gpt2` model for Causal Language Modeling and moving it to the selected device (GPU). This is the core PyTorch model object that will be fine-tuned."
      ],
      "metadata": {
        "id": "load-model"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "5Y3A1q6M1i5k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 10. Creating PyTorch DataLoaders",
        "",
        "For manual PyTorch training, I wrap the tokenized Hugging Face `datasets` objects into PyTorch `DataLoaders`. This utility handles batching, shuffling, and multi-process data loading for efficient GPU usage."
      ],
      "metadata": {
        "id": "create-dataloaders"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, default_collate\n",
        "\n",
        "batch_size = 4\n",
        "\n",
        "# Use default_collate for standard tensor stacking\n",
        "train_dataloader = DataLoader(\n",
        "    tokenized_ds[\"train\"], \n",
        "    shuffle=True, \n",
        "    batch_size=batch_size, \n",
        "    collate_fn=default_collate\n",
        "    )\n",
        "eval_dataloader = DataLoader(\n",
        "    tokenized_ds[\"test\"], \n",
        "    shuffle=False, \n",
        "    batch_size=batch_size, \n",
        "    collate_fn=default_collate\n",
        "    )"
      ],
      "metadata": {
        "id": "w3fU2R5T6EwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 11. Setting Up Optimization Components",
        "",
        "This is a core manual step: I define the **AdamW optimizer** (standard for transformers) and a **linear learning rate scheduler** with a warm-up. These control how the model's weights are updated and how the learning rate changes over time."
      ],
      "metadata": {
        "id": "setup-optimizer"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "from transformers import get_scheduler\n",
        "\n",
        "learning_rate = 5e-5\n",
        "optimizer = optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "num_training_steps = len(train_dataloader) * 1\n",
        "scheduler = get_scheduler(\n",
        "    name=\"linear\", \n",
        "    optimizer=optimizer, \n",
        "    num_warmup_steps=0, \n",
        "    num_training_steps=num_training_steps\n",
        "    )"
      ],
      "metadata": {
        "id": "S1yR5r4F8pQZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 12. Defining the Evaluation Function",
        "",
        "I define a helper function to calculate the average loss on the validation set after each training epoch. This uses `torch.no_grad()` for efficiency and to prevent unintended weight updates."
      ],
      "metadata": {
        "id": "define-eval"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, eval_dataloader):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    for batch in eval_dataloader:\n",
        "        # Move batch tensors to device\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        labels = input_ids.clone()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "        \n",
        "        total_loss += outputs.loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(eval_dataloader)\n",
        "    print(f\"Evaluation Loss: {avg_loss:.4f}\")\n",
        "    return avg_loss"
      ],
      "metadata": {
        "id": "O9kL3yJ4Y-rO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 13. Implementing the Manual PyTorch Training Loop",
        "",
        "This function is the core of the manual fine-tuning. It contains the essential steps: iterating through batches, calculating loss, performing the **backward pass** (`loss.backward()`), and updating weights (`optimizer.step()`), explicitly demonstrating low-level PyTorch control."
      ],
      "metadata": {
        "id": "manual-train-loop"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(model, train_dataloader, eval_dataloader, optimizer, scheduler, num_epochs=3):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch + 1}/{num_epochs}\")\n",
        "        total_loss = 0\n",
        "        for batch_idx, batch in enumerate(train_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            # Move batch tensors to device\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            attention_mask = batch[\"attention_mask\"].to(device)\n",
        "            labels = input_ids.clone()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            \n",
        "            # Optimization step\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            \n",
        "            if (batch_idx + 1) % 100 == 0:\n",
        "                print(f\"  Batch {batch_idx + 1}/{len(train_dataloader)} Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_train_loss = total_loss / len(train_dataloader)\n",
        "        print(f\"Average Training Loss for Epoch {epoch + 1}: {avg_train_loss:.4f}\")\n",
        "        \n",
        "        # Evaluate after each epoch\n",
        "        evaluate_model(model, eval_dataloader)\n",
        "        model.train() # Set back to train mode\n",
        "    \n",
        "    print(\"Manual fine-tuning complete.\")"
      ],
      "metadata": {
        "id": "T0wR7nI42-fR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 14. Executing the Manual Fine-Tuning",
        "",
        "I initiate the training process. I'm starting with a single epoch to quickly demonstrate the model's domain adaptation and my manual implementation skills."
      ],
      "metadata": {
        "id": "execute-training"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 1\n",
        "train_model(model=model, train_dataloader=train_dataloader, eval_dataloader=eval_dataloader, optimizer=optimizer, scheduler=scheduler, num_epochs=num_epochs)"
      ],
      "metadata": {
        "id": "B0qX1pL78aFz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 15. Qualitative Check: Defining Text Generation Function",
        "",
        "I define a helper function using the Hugging Face `pipeline` for text generation. This will be used to qualitatively inspect the model's new behavior after fine-tuning."
      ],
      "metadata": {
        "id": "define-generation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "def generate_text(model, tokenizer, prompt, max_length=256, temperature=0.7):\n",
        "    model.eval() # Switch to evaluation mode\n",
        "    \n",
        "    # Create a text generation pipeline\n",
        "    gen_pipeline = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        device=0 if torch.cuda.is_available() else -1 # Use GPU if available\n",
        "    )\n",
        "\n",
        "    # Generate text\n",
        "    generated = gen_pipeline(\n",
        "        prompt,\n",
        "        max_length=max_length,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        top_k=50,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.pad_token_id, # Must explicitly set pad_token_id\n",
        "        num_return_sequences=1\n",
        "    )\n",
        "    return generated[0]['generated_text']"
      ],
      "metadata": {
        "id": "T2yL4xH73kYp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 16. Qualitative Check: Running Text Generation",
        "",
        "I test the fine-tuned model's ability to generate a solution and reasoning based on a new problem statement, confirming its learned capabilities."
      ],
      "metadata": {
        "id": "execute-generation"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompt = \"Problem: Write a Python function that takes a list of integers and returns the sum of all odd numbers in the list.\\n###Solution:\"\n",
        "print(\"Generating text using fine-tuned model...\")\n",
        "generated_output = generate_text(model=model, tokenizer=tokenizer, prompt=test_prompt, max_length=150)\n",
        "print(\"---- Generated Output ----\")\n",
        "print(generated_output)"
      ],
      "metadata": {
        "id": "D2mJ4nK9a3Tf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 17. Saving and Pushing the Fine-Tuned Model to Hugging Face Hub",
        "",
        "I save the fine-tuned model and tokenizer locally and then use the `push_to_hub` utility to upload them to my public Hub account. This completes Phase 1 and makes the model available for subsequent steps (using the Hugging Face `Trainer` and DPO)."
      ],
      "metadata": {
        "id": "push-to-hub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "repo_name = \"my-gpt2-coding-finetuned-pytorch-manual\" # Choose a meaningful name\n",
        "\n",
        "model.save_pretrained(repo_name)\n",
        "tokenizer.save_pretrained(repo_name)\n",
        "\n",
        "# Push to Hub - requires being logged in via !huggingface-cli login\n",
        "print(f\"Pushing model and tokenizer to Hub under: {repo_name}\")\n",
        "tokenizer.push_to_hub(repo_name)\n",
        "model.push_to_hub(repo_name)"
      ],
      "metadata": {
        "id": "J1rK7sH2x9Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 18. Final Quantitative Evaluation",
        "",
        "I run the evaluation function one last time to confirm the final loss on the validation set after the completion of the entire manual training process."
      ],
      "metadata": {
        "id": "final-eval"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate_model(model=model,eval_dataloader=eval_dataloader)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mYHPTS-q4AG",
        "outputId": "153efca1-28e4-4e1d-c102-ba47e7639103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation Loss: 0.0139\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.0139"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
